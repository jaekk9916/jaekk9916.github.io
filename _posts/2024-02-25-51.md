---
title: Derivatives and Optimization
date: 2024-02-25 13:07:23 +0500
categories: [Math, Calculus for Machine Learning and Data Science]
tags: [Math, Calculus, Machine Learning, Data Science]
editor_options:
  markdown:
    wrap: sentence
    extensions: +tex_math_dollars

---

## Why is calculus important in machine learning?
Because we use derivatives to maximize or minimize a function 
=> this means finding the maximum or minimum value of a function

* When we want to find the model that best fits the data, we calculate the *loss function* and minimize it, and repeat the process over and over again

### Model training
The model starts with an arbitrary line => something that looks like $Wx + b$.
and then tweak the results to optimize the best possible prediction for the existing data points

#### Related problem types
- Linear Regression
- Classification
	- Sentiment analysis (results are expressed as happy/sad/joyful/etc.)

*Currently, machine learning problems are handled by many different mathematical techniques*.

### Derivative = Derivative
* Derivative is the instantaneous rate of change of a function.
* Derivative of a function at a point = slope of the tangent line at that point
* To find the maximum or minimum value of a function, you must find one of the points where the derivative goes to zero.

### How to represent derivatives
- Lagrange's Notation
- $y=f(x) \rightarrow f'(x)$
- Leibniz's Notation
	  $\frac{dy}{dx}=\frac{d}{dx}f(x)$

#### Derivatives of line of equation
```math
\begin{aligned}
f(x) &= ax + b \rightarrow f'(x) = a \\\\
\frac{\Delta y}{\Delta x} &= \frac{a(x+\Delta x) + b - (ax + b)}{\Delta x} \\\\
a\frac{\Delta x}{\Delta x} &= a 
\end{aligned}
```

#### Derivatives - Quadratic Equations
$$
\begin{aligned}
Quadratics: y = f(x) = x^2 \\\\
Slope: \frac{\Delta f}{\Delta x} = \frac{f(x+{\Delta x})-f(x)}{\Delta x} \\\\
\frac{{\Delta f}}{{\Delta x}}= \frac{(x+{\Delta x)^2}-x^2}{\Delta x} \\\\
=\frac{x^2+2x{\Delta x}+({\Delta x)^2-x^2}}{{\Delta x}} \\\\
=2x+{\Delta x}\\\\\\\\
as \,\,\,\,\, {\Delta x} \rightarrow 0 \\\\
f(x) = x^2 \rightarrow f'(x)=2x
\end{aligned}
$$

#### Derivatives - Higher degree polynomials
$$
\begin{aligned}
Cubic:y=f(x)=x^3\\\\
Slope:\frac{{\Delta f}}{{\Delta x}}=\frac{(x+{\Delta x})^3-(x^3)}{{\Delta x}}\\\\
\frac{{\Delta f}}{{\Delta x}}=(x+{\Delta x})^3-x^3{{\Delta x}}\\\\
=\frac{x^3+3x({\Delta x})^2+3x^2{\Delta x}+({\Delta x})^3-x^3}{{\Delta x}}\\\\
=3x{\Delta x}+3x^2+{\Delta x}^2\\\\\\\\
as \,\,\,\,\, {\Delta x} \rightarrow 0 \\\\
\,\,\,\,\,\,\,\, f(x)=x^3 \rightarrow f'(x)=3x^2
\end{aligned}
$$

#### Derivative of $\frac{1}{x}$

$$
\begin{aligned}
Inverse:y=f(x)=x^-1=\frac{1}{x}\\\\
Slope: \frac{{\Delta f}}{\Delta x}=\frac{f(x+{\Delta x})-f(x)}{{\Delta x}}\\\\
\frac{\Delta f}{\Delta x}=\frac{(x+{\Delta x})^-1-x^-1}{{\Delta x}}\\\\
=\frac{\frac{1}{x+{\Delta x}}-\frac{1}{x}}{{\Delta x}}\\\\
=\frac{\frac{x-(x+{\Delta x})}{(x+{\Delta x})x}}{{\Delta x}}\\\\
=-\frac{1}{x^2+x{\Delta x}}\\\\\\\\
as {\Delta x} \rightarrow 0\\\\
f(X)=x^-1 \rightarrow f'(x)=-x^-2
\end{aligned}
$$

